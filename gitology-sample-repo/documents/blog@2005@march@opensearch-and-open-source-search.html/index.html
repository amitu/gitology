<p><div style="clear:both;"></div>Was just reading about <a href="http://a9.com/-/company/whatsCool.jsp">A9</a>'s <a href="http://opensearch.a9.com/">OpenSearch</a> (<a href="http://opensearch.a9.com/-/search/moreColumns.jsp">demo</a>) when I remembered a writeup I had written in the years before the blogging became cool:<br /><blockquote>Was having lunch, and this thing stuck me. First I thought why don't Google do "SETI"(later realized they do something similer, but read on). People would obviously be willing to donate a few processor cycles to Google [we love Google!]. This I say because after all what a node at the Google's distributed systems is? quite close to my current system that is lying idle in my room while I am lunching. So for google it may make much more sense than for MSN search maybe [who probably is not using off the shelf components and have special requirements]. So I asked myself how will I (as Google) use this system? Well if people are willing to share bandwidth too, I may let the "donated nodes" fetch and index website [here by Index I mean compiling the raw text to some processed binary format, which will then further be used to search rank the pages at Google's site].<br /> <br />But then I realised it would be only a marginal thing, wont really change things much, and then got this idea: What if websites index themselves? After all we know the difference between ftp-ing a directory and ftp-ing tar.gz for the same directory(with probably 1000s of small files). How about a googlify.exe that will create google.index which will be placed like robot.txt?<br /> <br />Why?<br /> <br />* Can save a bit of bandwidth, and much time (checking 100-1000 pages per website vs 1/few, even if all you did is to check the last modification timestamp).<br />* Job of sanity checking and indexing can be distributed, reducing the load on the google servers (and hence much more detailed algos can be employed).<br /> <br />Why would a website do that?<br /> <br />Everyone wants to be ranked high. Everyone wants their latest content to be indexed by google soon. Everyone wants to optimize their bandwidth utilization. So lets say there is something like <a href="http://ping.google.com/">ping.google.com</a>, and  you can do a: <a href="http://ping.google.com/?host=http://www.retrolabs.com/google.index">  http://ping.google.com/?host=http://www.retrolabs.com/google.index</a> after lets say when something on www.retrolabs.com gets modified and new google.index has been created, that will give the webmaster great deal of control on how quickly site shows up in google search results for changes just made.  </blockquote><br /><br />[Small clearification: First question is how can you trust a google.index file to be authentic? But that you might be able to control by doing crazy/smart things in Googlify.exe that site owners download from Google. Or everytime a googlify.exe "pings" the mothership, mothership "signals" random googlify.exe-s to check the authenticity of the index, and penalize a bad googlify.exe if required.]<br /><br />I don't know if Google would do it, or if there would be the need of for them, they have immense infrastructure, but this is how opensource search engines will have to be if there is to be any. <br /> <br /><div style="clear:both; padding-bottom: 0.25em;"></div></p>